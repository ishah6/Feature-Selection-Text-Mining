{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment5_IS.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1hDk9hDvOH7",
        "colab_type": "code",
        "outputId": "daaeb915-d0da-4df3-9a4f-8ad0276f98f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#NLTK-------------------------------\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Import libraries for feature \n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "#Change current working directory to gdrive\n",
        "%cd /gdrive\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMwGQK7KAd7T",
        "colab_type": "code",
        "outputId": "4423d84b-a96e-4412-cb53-14e3744070b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#Read files\n",
        "textfile = r'/gdrive/My Drive/CIS508/Assignment-5/Comments.csv'\n",
        "textData = pd.read_csv(textfile) #creates a dataframe\n",
        "\n",
        "CustInfofile = r'/gdrive/My Drive/CIS508/Assignment-5/Customers.csv'\n",
        "CustInfoData = pd.read_csv(CustInfofile)  #creates a dataframe\n",
        "\n",
        "print(textData.shape)\n",
        "print(CustInfoData.shape)\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2070, 2)\n",
            "(2070, 17)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWOTk6C1Ao45",
        "colab_type": "code",
        "outputId": "61ef7228-5888-44cf-fef4-b7d7ca2b2a40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "source": [
        "#Extract target column from Customer Info file\n",
        "Y_Train = CustInfoData[\"TARGET\"]\n",
        "X_Train = CustInfoData.drop(columns=[\"TARGET\"]) #extracting training data without the target column\n",
        "                     \n",
        "print(X_Train.shape)\n",
        "print(textData.shape)\n",
        "textData.head()\n",
        "print(X_Train)\n",
        "print(Y_Train)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2070, 16)\n",
            "(2070, 2)\n",
            "        ID Sex Status  ...  Paymethod  LocalBilltype LongDistanceBilltype\n",
            "0        1   F      S  ...         CC         Budget       Intnl_discount\n",
            "1        6   M      M  ...         CH      FreeLocal             Standard\n",
            "2        8   M      M  ...         CC      FreeLocal             Standard\n",
            "3       11   M      S  ...         CC         Budget             Standard\n",
            "4       14   F      M  ...         CH         Budget       Intnl_discount\n",
            "...    ...  ..    ...  ...        ...            ...                  ...\n",
            "2065  3821   F      S  ...         CC      FreeLocal             Standard\n",
            "2066  3822   F      S  ...       Auto         Budget             Standard\n",
            "2067  3823   F      M  ...         CH         Budget             Standard\n",
            "2068  3824   F      M  ...         CC      FreeLocal             Standard\n",
            "2069  3825   F      S  ...         CC      FreeLocal             Standard\n",
            "\n",
            "[2070 rows x 16 columns]\n",
            "0       Cancelled\n",
            "1         Current\n",
            "2         Current\n",
            "3         Current\n",
            "4       Cancelled\n",
            "          ...    \n",
            "2065    Cancelled\n",
            "2066    Cancelled\n",
            "2067    Cancelled\n",
            "2068    Cancelled\n",
            "2069    Cancelled\n",
            "Name: TARGET, Length: 2070, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NHZBJBRIIhC",
        "colab_type": "text"
      },
      "source": [
        "**SNOWBALL STEMMER**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuWYNz2Ep17l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use English stemmer.\n",
        "Stemmer1 = SnowballStemmer(\"english\")\n",
        "#Tokenize - Split the sentences to lists of words\n",
        "textData['CommentsTokenized'] = textData['Comments'].apply(word_tokenize)\n",
        "export_csv = textData.to_csv(r'/gdrive/My Drive/CIS508/Assignment-5/TextDataTokenized_snow.csv')\n",
        "\n",
        "#Now do stemming - create a new dataframe to store stemmed version\n",
        "NewTextData1=pd.DataFrame()\n",
        "NewTextData1=textData.drop(columns=[\"CommentsTokenized\",\"Comments\"])\n",
        "NewTextData1['CommentsTokenizedStemmed'] = textData['CommentsTokenized'].apply(lambda x: [Stemmer1.stem(y) for y in x]) # Stem every word.\n",
        "export_csv = NewTextData1.to_csv(r'/gdrive/My Drive/CIS508/Assignment-5/NewTextDataTS_snow.csv')\n",
        "\n",
        "#Join stemmed strings\n",
        "NewTextData1['CommentsTokenizedStemmed'] = NewTextData1['CommentsTokenizedStemmed'].apply(lambda x: \" \".join(x))\n",
        "export_csv = NewTextData1.to_csv(r'/gdrive/My Drive/CIS508/Assignment-5/NewTextData-Joined_snow.csv')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiBguQloljam",
        "colab_type": "code",
        "outputId": "ff750372-7a72-4fcc-eb85-e3e1c0d8df1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "#Do Bag-Of-Words model - Term - Document Matrix\n",
        "#Learn the vocabulary dictionary and return term-document matrix.\n",
        "#count_vect = CountVectorizer(stop_words=None)\n",
        "count_vect = CountVectorizer(stop_words='english',lowercase=False)\n",
        "TD_counts1 = count_vect.fit_transform(NewTextData1.CommentsTokenizedStemmed)\n",
        "#print(TD_counts1.shape)\n",
        "TD_counts1.dtype\n",
        "print(count_vect.get_feature_names())\n",
        "print(\"No. of stem words in SNOWBALL stemmer is : \",len(count_vect.get_feature_names()))\n",
        "#print(TD_counts1)\n",
        "DF_TD_Counts=pd.DataFrame(TD_counts1.toarray())\n",
        "print(DF_TD_Counts)\n",
        "export_csv = DF_TD_Counts.to_csv(r'/gdrive/My Drive/CIS508/Assignment-5/TD_counts-TokenizedStemmed_snow.csv')\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['3399', '3g', 'abysm', 'access', 'accessori', 'adapt', 'add', 'addit', 'additon', 'address', 'adit', 'adress', 'advertis', 'afraid', 'alway', 'angel', 'angri', 'ani', 'anoth', 'anyth', 'anytim', 'area', 'asap', 'ask', 'bad', 'basic', 'bateri', 'batteri', 'becaus', 'believ', 'better', 'bigger', 'book', 'bought', 'brain', 'bring', 'built', 'busi', 'button', 'buy', 'cancel', 'cancer', 'car', 'care', 'carrier', 'caus', 'cc', 'cell', 'certain', 'chang', 'charg', 'charger', 'check', 'chip', 'citi', 'claim', 'cleariti', 'cold', 'comapr', 'compani', 'compar', 'competit', 'complain', 'complaint', 'concept', 'connect', 'consisit', 'consist', 'constan', 'contact', 'continu', 'contract', 'correct', 'cost', 'coupl', 'cover', 'coverag', 'creat', 'credit', 'cstmer', 'cstmr', 'current', 'cust', 'custom', 'customr', 'date', 'day', 'dead', 'decent', 'defect', 'deo', 'did', 'die', 'differ', 'difficult', 'digiti', 'direct', 'disabl', 'doe', 'don', 'dont', 'drop', 'dure', 'easier', 'effect', 'encount', 'end', 'enemi', 'equip', 'everytim', 'everywher', 'evrey', 'exact', 'expect', 'expir', 'explain', 'facepl', 'fals', 'famili', 'featur', 'fed', 'figur', 'fine', 'fix', 'forev', 'forward', 'friend', 'function', 'furthermor', 'futur', 'gave', 'goat', 'good', 'great', 'gsm', 'handset', 'happi', 'hard', 'hate', 'hear', 'heard', 'help', 'higher', 'highway', 'hochi', 'hole', 'home', 'hope', 'horribl', 'hous', 'implement', 'improv', 'inadequ', 'includ', 'info', 'inform', 'ing', 'internet', 'intersect', 'issu', 'june', 'just', 'kid', 'kno', 'know', 'lame', 'later', 'lctn', 'learn', 'leroy', 'like', 'line', 'list', 'local', 'locat', 'locatn', 'long', 'los', 'lost', 'lot', 'love', 'major', 'make', 'manag', 'mani', 'manual', 'market', 'mean', 'messag', 'metropolitian', 'minut', 'misl', 'mistak', 'model', 'momma', 'mr', 'napeleon', 'near', 'nearest', 'need', 'network', 'new', 'news', 'notic', 'number', 'numer', 'offer', 'old', 'om', 'open', 'option', 'ori', 'ot', 'outbound', 'pass', 'pay', 'pda', 'peopl', 'perform', 'person', 'phone', 'piec', 'plan', 'pleas', 'point', 'polici', 'poor', 'possibl', 'probabl', 'problem', 'proper', 'provid', 'provis', 'purpos', 'rate', 'rater', 'realiz', 'realli', 'reason', 'receiv', 'recept', 'recption', 'reenter', 'refer', 'relat', 'rep', 'replac', 'respect', 'result', 'rid', 'right', 'ring', 'roam', 'roll', 'rubbish', 'rude', 'said', 'sale', 'say', 'screen', 'self', 'send', 'servic', 'shitti', 'shut', 'sign', 'signal', 'signific', 'simm', 'simpli', 'sinc', 'site', 'slow', 'sold', 'someon', 'sometim', 'soon', 'speak', 'speed', 'start', 'static', 'stole', 'store', 'stuff', 'stupid', 'substant', 'subtract', 'suck', 'suggest', 'supervisor', 'support', 'sure', 'surpris', 'suspect', 'suspend', 'switch', 'teach', 'technic', 'tell', 'terribl', 'test', 'text', 'think', 'thought', 'ticket', 'till', 'time', 'tire', 'today', 'toilet', 'told', 'tone', 'tower', 'transeff', 'transf', 'transfer', 'travel', 'tri', 'trust', 'turn', 'uncomfort', 'understand', 'unhappi', 'unlimit', 'unreli', 'unwil', 'upset', 'usag', 'use', 'useless', 'valu', 'veri', 'vm', 'wa', 'wait', 'want', 'wast', 'way', 'weak', 'web', 'websit', 'week', 'whi', 'wife', 'wish', 'wll', 'wold', 'work', 'wors', 'worst', 'wrong', 'xvyx', 'year', 'york']\n",
            "No. of stem words in SNOWBALL stemmer is :  354\n",
            "      0    1    2    3    4    5    6    ...  347  348  349  350  351  352  353\n",
            "0       0    0    0    0    0    0    0  ...    1    0    0    0    0    0    0\n",
            "1       0    0    0    0    1    0    0  ...    0    0    0    0    0    0    0\n",
            "2       0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "3       0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "4       0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
            "2065    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "2066    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "2067    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "2068    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "2069    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "\n",
            "[2070 rows x 354 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30UtRc5sIQ6L",
        "colab_type": "text"
      },
      "source": [
        "**PORTER STEMMER**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1HgxPMCIcDN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use English stemmer.\n",
        "Stemmer2 = PorterStemmer()\n",
        "#Tokenize - Split the sentences to lists of words\n",
        "textData['CommentsTokenized'] = textData['Comments'].apply(word_tokenize)\n",
        "\n",
        "export_csv = textData.to_csv(r'/gdrive/My Drive/CIS508/Assignment-5/TextDataTokenized_porter.csv')\n",
        "\n",
        "#Now do stemming - create a new dataframe to store stemmed version\n",
        "NewTextData2=pd.DataFrame()\n",
        "NewTextData2=textData.drop(columns=[\"CommentsTokenized\",\"Comments\"])\n",
        "NewTextData2['CommentsTokenizedStemmed'] = textData['CommentsTokenized'].apply(lambda x: [Stemmer2.stem(y) for y in x]) # Stem every word.\n",
        "\n",
        "export_csv = NewTextData2.to_csv(r'/gdrive/My Drive/CIS508/Assignment-5/NewTextDataTS_porter.csv')\n",
        "\n",
        "#Join stemmed strings\n",
        "NewTextData2['CommentsTokenizedStemmed'] = NewTextData2['CommentsTokenizedStemmed'].apply(lambda x: \" \".join(x))\n",
        "\n",
        "export_csv = NewTextData2.to_csv(r'/gdrive/My Drive/CIS508/Assignment-5/NewTextData-Joined_porter.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul7FhKoenMb8",
        "colab_type": "code",
        "outputId": "06303e30-2450-413a-ca4b-4905835ee9a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "#Do Bag-Of-Words model - Term - Document Matrix\n",
        "#Learn the vocabulary dictionary and return term-document matrix.\n",
        "#count_vect = CountVectorizer(stop_words=None)\n",
        "count_vect = CountVectorizer(stop_words='english',lowercase=False)\n",
        "TD_counts2 = count_vect.fit_transform(NewTextData2.CommentsTokenizedStemmed)\n",
        "#print(TD_counts2.shape)\n",
        "TD_counts2.dtype\n",
        "print(count_vect.get_feature_names())\n",
        "print(\"No. of stem words in PORTER stemmer is : \",len(count_vect.get_feature_names()))\n",
        "#print(TD_counts1)\n",
        "DF_TD_Counts=pd.DataFrame(TD_counts2.toarray())\n",
        "print(DF_TD_Counts)\n",
        "export_csv = DF_TD_Counts.to_csv(r'/gdrive/My Drive/CIS508/Assignment-5/TD_counts-TokenizedStemmed_porter.csv')"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['3399', '3g', 'As', 'CC', 'He', 'If', 'In', 'Is', 'It', 'We', 'abysm', 'access', 'accessori', 'adapt', 'add', 'addit', 'additon', 'address', 'adit', 'adress', 'advertis', 'afraid', 'alway', 'angel', 'angri', 'ani', 'anoth', 'anyth', 'anytim', 'area', 'asap', 'ask', 'bad', 'basic', 'bateri', 'batteri', 'becaus', 'believ', 'better', 'bigger', 'book', 'bought', 'brain', 'bring', 'built', 'busi', 'button', 'buy', 'cancel', 'cancer', 'car', 'care', 'carrier', 'caus', 'cc', 'cell', 'certain', 'chang', 'charg', 'charger', 'check', 'chip', 'citi', 'claim', 'cleariti', 'cold', 'comapr', 'compani', 'compar', 'competit', 'complain', 'complaint', 'concept', 'connect', 'consisit', 'consist', 'constanli', 'contact', 'continu', 'contract', 'correct', 'cost', 'coupl', 'cover', 'coverag', 'creat', 'credit', 'cstmer', 'cstmr', 'current', 'cust', 'custom', 'customr', 'date', 'day', 'dead', 'decent', 'defect', 'deo', 'did', 'die', 'differ', 'difficult', 'digiti', 'direct', 'disabl', 'doe', 'don', 'dont', 'drop', 'dure', 'easier', 'effect', 'encount', 'end', 'enemi', 'equip', 'everytim', 'everywher', 'evrey', 'exactli', 'expect', 'expir', 'explain', 'facepl', 'fals', 'famili', 'featur', 'fed', 'figur', 'fine', 'fix', 'forev', 'forward', 'friend', 'function', 'furthermor', 'futur', 'gave', 'goat', 'good', 'great', 'gsm', 'ha', 'handset', 'happi', 'hard', 'hardli', 'hate', 'hear', 'heard', 'help', 'hi', 'higher', 'highway', 'hochi', 'hole', 'home', 'hope', 'horribl', 'hous', 'implement', 'improv', 'inadequ', 'includ', 'info', 'inform', 'ing', 'internet', 'intersect', 'issu', 'june', 'just', 'kid', 'kno', 'know', 'lame', 'later', 'lctn', 'learn', 'leroy', 'like', 'line', 'list', 'lo', 'local', 'locat', 'locatn', 'long', 'lost', 'lot', 'love', 'major', 'make', 'manag', 'mani', 'manual', 'market', 'mean', 'messag', 'metropolitian', 'minut', 'misl', 'mistak', 'model', 'momma', 'mr', 'napeleon', 'near', 'nearest', 'need', 'network', 'new', 'news', 'notic', 'number', 'numer', 'offer', 'old', 'om', 'open', 'option', 'ori', 'ot', 'outbound', 'pass', 'pay', 'pda', 'peopl', 'perform', 'person', 'phone', 'piec', 'plan', 'pleas', 'point', 'polici', 'poor', 'possibl', 'probabl', 'problem', 'properli', 'provid', 'provis', 'purpos', 'rate', 'rater', 'realiz', 'realli', 'reason', 'receiv', 'recept', 'recption', 'reenter', 'refer', 'relat', 'rep', 'replac', 'respect', 'result', 'rid', 'right', 'ring', 'roam', 'roll', 'rubbish', 'rude', 'said', 'sale', 'say', 'screen', 'self', 'send', 'servic', 'shitti', 'shut', 'sign', 'signal', 'significantli', 'simm', 'simpli', 'sinc', 'site', 'slow', 'sold', 'someon', 'sometim', 'soon', 'speak', 'speed', 'start', 'static', 'stole', 'store', 'stuff', 'stupid', 'substant', 'subtract', 'suck', 'suggest', 'supervisor', 'support', 'sure', 'surpris', 'suspect', 'suspend', 'switch', 'teach', 'technic', 'tell', 'terribl', 'test', 'text', 'thi', 'think', 'thought', 'ticket', 'till', 'time', 'tire', 'today', 'toilet', 'told', 'tone', 'tower', 'transeff', 'transf', 'transfer', 'travel', 'tri', 'trust', 'turn', 'uncomfort', 'understand', 'unhappi', 'unlimit', 'unreli', 'unwil', 'upset', 'usag', 'use', 'useless', 'valu', 'veri', 'vm', 'wa', 'wait', 'want', 'wast', 'way', 'weak', 'web', 'websit', 'week', 'whi', 'wife', 'wish', 'wll', 'wold', 'work', 'wors', 'worst', 'wrong', 'xvyx', 'year', 'york']\n",
            "No. of stem words in PORTER stemmer is :  366\n",
            "      0    1    2    3    4    5    6    ...  359  360  361  362  363  364  365\n",
            "0       0    0    0    0    0    0    0  ...    1    0    0    0    0    0    0\n",
            "1       0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "2       0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "3       0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "4       0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
            "2065    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "2066    0    0    0    0    2    0    0  ...    0    0    0    0    0    0    0\n",
            "2067    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "2068    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "2069    0    0    0    0    2    0    0  ...    0    0    0    0    0    0    0\n",
            "\n",
            "[2070 rows x 366 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA-CHr-PLakM",
        "colab_type": "text"
      },
      "source": [
        "**LANCASTER STEMMER**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChZDd9YzLexg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use English stemmer.\n",
        "Stemmer3 = LancasterStemmer()\n",
        "#Tokenize - Split the sentences to lists of words\n",
        "textData['CommentsTokenized'] = textData['Comments'].apply(word_tokenize)\n",
        "\n",
        "export_csv = textData.to_csv(r'/gdrive/My Drive/CIS508/Assignment-5/TextDataTokenized_lancaster.csv')\n",
        "\n",
        "#Now do stemming - create a new dataframe to store stemmed version\n",
        "NewTextData=pd.DataFrame()\n",
        "NewTextData=textData.drop(columns=[\"CommentsTokenized\",\"Comments\"])\n",
        "NewTextData['CommentsTokenizedStemmed'] = textData['CommentsTokenized'].apply(lambda x: [Stemmer3.stem(y) for y in x]) # Stem every word.\n",
        "\n",
        "export_csv = NewTextData.to_csv(r'/gdrive/My Drive/CIS508/Assignment-5/NewTextDataTS_lancaster.csv')\n",
        "\n",
        "#Join stemmed strings\n",
        "NewTextData['CommentsTokenizedStemmed'] = NewTextData['CommentsTokenizedStemmed'].apply(lambda x: \" \".join(x))\n",
        "\n",
        "export_csv = NewTextData.to_csv(r'/gdrive/My Drive/CIS508/Assignment-5/NewTextData-Joined_lancaster.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WG1bQptrnWTi",
        "colab_type": "code",
        "outputId": "4d5daecc-74c2-4f35-e0c4-aa780fb043c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "#Do Bag-Of-Words model - Term - Document Matrix\n",
        "#Learn the vocabulary dictionary and return term-document matrix.\n",
        "#count_vect = CountVectorizer(stop_words=None)\n",
        "count_vect = CountVectorizer(stop_words='english',lowercase=False)\n",
        "TD_counts3 = count_vect.fit_transform(NewTextData.CommentsTokenizedStemmed)\n",
        "#print(TD_counts3.shape)\n",
        "TD_counts3.dtype\n",
        "print(count_vect.get_feature_names())\n",
        "print(\"No. of stem words in LANCASTER stemmer is : \",len(count_vect.get_feature_names()))\n",
        "#print(TD_counts3)\n",
        "DF_TD_Counts=pd.DataFrame(TD_counts3.toarray())\n",
        "print(DF_TD_Counts)\n",
        "export_csv = DF_TD_Counts.to_csv(r'/gdrive/My Drive/CIS508/Assignment-5/TD_counts-TokenizedStemmed_lancaster.csv')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['3399', '3g', 'abysm', 'access', 'ad', 'adapt', 'addit', 'additon', 'address', 'adit', 'adress', 'advert', 'afraid', 'aft', 'al', 'alway', 'angel', 'angry', 'anoth', 'anyth', 'anytim', 'ar', 'asap', 'ask', 'bad', 'bas', 'batery', 'battery', 'becaus', 'believ', 'bet', 'big', 'bil', 'book', 'bought', 'brain', 'bring', 'built', 'busy', 'button', 'buy', 'cal', 'cancel', 'car', 'carry', 'caus', 'cc', 'cel', 'certain', 'chang', 'charg', 'check', 'chip', 'city', 'claim', 'clear', 'cold', 'comapr', 'comp', 'company', 'competit', 'complain', 'complaint', 'conceiv', 'connect', 'consisit', 'consist', 'const', 'contact', 'continu', 'contract', 'correct', 'cost', 'coupl', 'cov', 'cre', 'credit', 'cstmer', 'cstmr', 'cur', 'cust', 'custom', 'customr', 'dat', 'day', 'dead', 'dec', 'defect', 'deo', 'did', 'die', 'diff', 'difficult', 'digit', 'direct', 'dis', 'doe', 'don', 'dont', 'drop', 'dur', 'dying', 'easy', 'effect', 'encount', 'end', 'enemy', 'equip', 'ev', 'everytim', 'everywh', 'evrey', 'exact', 'expect', 'expir', 'explain', 'facepl', 'fals', 'famy', 'feat', 'fed', 'fig', 'fin', 'fix', 'forev', 'forward', 'friend', 'funct', 'furtherm', 'fut', 'gav', 'giv', 'goat', 'going', 'good', 'gre', 'gsm', 'handset', 'happy', 'hard', 'hat', 'hav', 'hear', 'heard', 'help', 'high', 'highway', 'hochy', 'hol', 'hom', 'hop', 'horr', 'hous', 'impl', 'improv', 'inadequ', 'includ', 'info', 'inform', 'ing', 'internet', 'intersect', 'issu', 'jun', 'just', 'kid', 'kno', 'know', 'lam', 'lat', 'lctn', 'learn', 'leroy', 'lik', 'lin', 'list', 'loc', 'locatn', 'long', 'los', 'lost', 'lot', 'lov', 'mad', 'maj', 'mak', 'man', 'market', 'mean', 'mess', 'metropolit', 'minut', 'misl', 'mistak', 'model', 'momm', 'mor', 'mov', 'mr', 'napeleon', 'near', 'nearest', 'nee', 'network', 'nev', 'new', 'num', 'numb', 'oft', 'old', 'omer', 'op', 'opt', 'ory', 'ot', 'oth', 'outbound', 'ov', 'pass', 'pay', 'pda', 'peopl', 'perform', 'person', 'phon', 'piec', 'plan', 'pleas', 'point', 'policy', 'poor', 'poss', 'prob', 'problem', 'prop', 'provid', 'purpos', 'rat', 'real', 'reason', 'receiv', 'recpt', 'reent', 'refer', 'rel', 'rep', 'replac', 'respect', 'result', 'rid', 'right', 'ring', 'roam', 'rol', 'rub', 'rud', 'said', 'sal', 'sam', 'say', 'screening', 'self', 'send', 'serv', 'shitty', 'shut', 'sign', 'sim', 'simply', 'sint', 'sit', 'slow', 'sold', 'som', 'someon', 'sometim', 'soon', 'speak', 'spee', 'start', 'stat', 'stil', 'stol', 'stor', 'stuff', 'stupid', 'subst', 'subtract', 'suck', 'suggest', 'superv', 'support', 'sur', 'surpr', 'suspect', 'suspend', 'switch', 'tak', 'teach', 'techn', 'tel', 'terr', 'test', 'text', 'ther', 'thes', 'thi', 'think', 'thos', 'thought', 'ticket', 'til', 'tim', 'tir', 'today', 'toilet', 'told', 'ton', 'tow', 'transeff', 'transf', 'transfer', 'travel', 'tri', 'trust', 'try', 'turn', 'uncomfort', 'understand', 'unhappy', 'unlimit', 'unrely', 'unwil', 'upset', 'useless', 'valu', 'vm', 'wa', 'wait', 'want', 'wast', 'way', 'weak', 'web', 'websit', 'week', 'wel', 'wer', 'wher', 'wheth', 'whol', 'wif', 'wil', 'wish', 'wll', 'wold', 'work', 'wors', 'worst', 'wrong', 'xvyx', 'year', 'yo', 'york']\n",
            "No. of stem words in LANCASTER stemmer is :  364\n",
            "      0    1    2    3    4    5    6    ...  357  358  359  360  361  362  363\n",
            "0       0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "1       0    0    0    1    0    0    0  ...    0    0    0    0    0    0    0\n",
            "2       0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "3       0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "4       0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
            "2065    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "2066    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "2067    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "2068    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "2069    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "\n",
            "[2070 rows x 364 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8YEFIktf2Zj4"
      },
      "source": [
        "**TF-IDF Matrix on best Stemmer method**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pd8TZYnAxQbP",
        "colab_type": "code",
        "outputId": "255b0584-e055-4f9f-941a-5e086854d62d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "source": [
        "#Compute TF-IDF Matrix\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_Train_tfidf = tfidf_transformer.fit_transform(TD_counts1)\n",
        "print(X_Train_tfidf.shape)\n",
        "DF_TF_IDF=pd.DataFrame(X_Train_tfidf.toarray())\n",
        "print(DF_TF_IDF)\n",
        "export_csv= DF_TF_IDF.to_csv(r'/gdrive/My Drive/CIS508/Assignment-5/TFIDF_counts_porter.csv')\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2070, 354)\n",
            "      0    1    2    3        4    5    ...  348  349  350  351  352  353\n",
            "0     0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "1     0.0  0.0  0.0  0.0  0.27568  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2     0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "3     0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "4     0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "...   ...  ...  ...  ...      ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
            "2065  0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2066  0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2067  0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2068  0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2069  0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "\n",
            "[2070 rows x 354 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fq_7eeQOz7xk",
        "colab_type": "code",
        "outputId": "a638e7ac-9ca2-444e-944a-c8673cb9d54a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "#Merge files\n",
        "\n",
        "print(CustInfoData.shape)\n",
        "\n",
        "combined=pd.concat([CustInfoData,DF_TF_IDF_SelectedFeatures], axis=1)\n",
        "print(combined.shape)\n",
        "print(combined)\n",
        "#export_csv= combined.to_csv(r'/gdrive/My Drive/CIS508/Combined-Cust+TFIDF+SelectedFeatures.csv')\n",
        "\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2070, 17)\n",
            "(2070, 67)\n",
            "        ID Sex Status  Children  Est_Income  ...   45        46   47   48   49\n",
            "0        1   F      S         1    38000.00  ...  0.0  0.000000  0.0  0.0  0.0\n",
            "1        6   M      M         2    29616.00  ...  0.0  0.000000  0.0  0.0  0.0\n",
            "2        8   M      M         0    19732.80  ...  0.0  0.000000  0.0  0.0  0.0\n",
            "3       11   M      S         2       96.33  ...  0.0  0.000000  0.0  0.0  0.0\n",
            "4       14   F      M         2    52004.80  ...  0.0  0.348322  0.0  0.0  0.0\n",
            "...    ...  ..    ...       ...         ...  ...  ...       ...  ...  ...  ...\n",
            "2065  3821   F      S         0    78851.30  ...  0.0  0.000000  0.0  0.0  0.0\n",
            "2066  3822   F      S         1    17540.70  ...  0.0  0.000000  0.0  0.0  0.0\n",
            "2067  3823   F      M         0    83891.90  ...  0.0  0.000000  0.0  0.0  0.0\n",
            "2068  3824   F      M         2    28220.80  ...  0.0  0.000000  0.0  0.0  0.0\n",
            "2069  3825   F      S         0    28589.10  ...  0.0  0.000000  0.0  0.0  0.0\n",
            "\n",
            "[2070 rows x 67 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPmJYiWrPkRG",
        "colab_type": "code",
        "outputId": "929e5d15-c1a5-480a-ce47-52f339041e4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#Do one Hot encoding for categorical features\n",
        "categorical_features = [\"Sex\",\"Status\",\"Car_Owner\",\"Paymethod\",\"LocalBilltype\",\"LongDistanceBilltype\"]\n",
        "#X_cat = combined.select_dtypes(exclude=['int','float64'])\n",
        "print(categorical_features)\n",
        "combined_one_hot = pd.get_dummies(combined,columns=categorical_features)\n",
        "print(combined_one_hot.shape)\n",
        "export_csv= combined_one_hot.to_csv(r'/gdrive/My Drive/CIS508/Assignment-5/combined_one_hot.csv')\n",
        "\n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Sex', 'Status', 'Car_Owner', 'Paymethod', 'LocalBilltype', 'LongDistanceBilltype']\n",
            "(2070, 75)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tuJEeqX6T-p",
        "colab_type": "text"
      },
      "source": [
        "**Feature selection: Filter method**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2owIUD6_eYO",
        "colab_type": "code",
        "outputId": "4edc655f-b299-47b1-8291-c1b2eb4dc7ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "#Feature selection\n",
        "new_DF_TF_IDF = SelectKBest(score_func=chi2, k=50).fit_transform(DF_TF_IDF,Y_Train)\n",
        "new_DF_TF_IDF.shape\n",
        "\n",
        "DF_TF_IDF_SelectedFeatures= pd.DataFrame(new_DF_TF_IDF)\n",
        "print(DF_TF_IDF_SelectedFeatures)\n",
        "#print(DF_TF_IDF_SelectedFeatures.columns)\n",
        "\n",
        "\n",
        "#export_csv= DF_TF_IDF_SelectedFeatures.to_csv(r'/gdrive/My Drive/CIS508/TFIDF_counts-Selected Features.csv')\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            0    1    2    3         4         5   ...   44   45   46   47   48   49\n",
            "0     0.000000  0.0  0.0  0.0  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "1     0.000000  0.0  0.0  0.0  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2     0.000000  0.0  0.0  0.0  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "3     0.000000  0.0  0.0  0.0  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "4     0.000000  0.0  0.0  0.0  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "...        ...  ...  ...  ...       ...       ...  ...  ...  ...  ...  ...  ...  ...\n",
            "2065  0.000000  0.0  0.0  0.0  0.000000  0.446161  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2066  0.000000  0.0  0.0  0.0  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2067  0.000000  0.0  0.0  0.0  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2068  0.772949  0.0  0.0  0.0  0.545354  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2069  0.000000  0.0  0.0  0.0  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "\n",
            "[2070 rows x 50 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJEWj_witnih",
        "colab_type": "text"
      },
      "source": [
        "**Feature selection: Filter method: Splitting Train and Test data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guKOqjvgtmob",
        "colab_type": "code",
        "outputId": "e6aaf7cd-9ad0-494c-c412-8cb2566044ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "source": [
        "X_Train1, X_Test1, Y_Train1, Y_Test1 = train_test_split(combined_one_hot.drop(columns=[\"TARGET\"]), \n",
        "                                                        combined_one_hot[\"TARGET\"], \n",
        "                                                        test_size=0.20, random_state=42)\n",
        "print('Training dataset shape:', X_Train1.shape, Y_Train1)\n",
        "print('Testing dataset shape:', X_Test1.shape, Y_Test1)\n"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training dataset shape: (1656, 74) 849       Current\n",
            "1043    Cancelled\n",
            "175       Current\n",
            "1228      Current\n",
            "538     Cancelled\n",
            "          ...    \n",
            "1638      Current\n",
            "1095      Current\n",
            "1130      Current\n",
            "1294      Current\n",
            "860       Current\n",
            "Name: TARGET, Length: 1656, dtype: object\n",
            "Testing dataset shape: (414, 74) 1181      Current\n",
            "69        Current\n",
            "351     Cancelled\n",
            "1163    Cancelled\n",
            "429       Current\n",
            "          ...    \n",
            "1532      Current\n",
            "1671      Current\n",
            "416       Current\n",
            "2023      Current\n",
            "1428      Current\n",
            "Name: TARGET, Length: 414, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geVCLka8xxjf",
        "colab_type": "code",
        "outputId": "2971a8d8-7fe6-4dc4-a41f-941099585215",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "#Construct a Random Forest Classifier on text data\n",
        "clf=RandomForestClassifier()\n",
        "RF_text = clf.fit(X_Train1,Y_Train1)\n",
        "print(\"Accuracy score (training): {0:.6f}\".format(clf.score(X_Test1, Y_Test1)))\n",
        "rf_predictions = clf.predict(X_Test1)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(Y_Test1, rf_predictions))\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(Y_Test1, rf_predictions))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score (training): 0.842995\n",
            "Confusion Matrix:\n",
            "[[124  33]\n",
            " [ 32 225]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cancelled       0.79      0.79      0.79       157\n",
            "     Current       0.87      0.88      0.87       257\n",
            "\n",
            "    accuracy                           0.84       414\n",
            "   macro avg       0.83      0.83      0.83       414\n",
            "weighted avg       0.84      0.84      0.84       414\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUFl6GsD1-b7",
        "colab_type": "code",
        "outputId": "1ab64754-0005-4d5a-d371-68b13ba0514d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "#Construct a  Decision Tree Classifier on text data\n",
        "clf=DecisionTreeClassifier()\n",
        "RF_text = clf.fit(X_Train1,Y_Train1)\n",
        "print(\"Accuracy score (training): {0:.6f}\".format(clf.score(X_Test1, Y_Test1)))\n",
        "rf_predictions = clf.predict(X_Test1)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(Y_Test1, rf_predictions))\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(Y_Test1, rf_predictions))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score (training): 0.821256\n",
            "Confusion Matrix:\n",
            "[[122  35]\n",
            " [ 39 218]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cancelled       0.76      0.78      0.77       157\n",
            "     Current       0.86      0.85      0.85       257\n",
            "\n",
            "    accuracy                           0.82       414\n",
            "   macro avg       0.81      0.81      0.81       414\n",
            "weighted avg       0.82      0.82      0.82       414\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Var6m43ccID",
        "colab_type": "code",
        "outputId": "7993f24c-d0a5-46f5-ab8a-0f2f56278601",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "#Construct a GRADIENT BOOSTING Classifier on text data\n",
        "clf=GradientBoostingClassifier()\n",
        "RF_text = clf.fit(X_Train1,Y_Train1)\n",
        "print(\"Accuracy score (training): {0:.6f}\".format(clf.score(X_Test1, Y_Test1)))\n",
        "rf_predictions = clf.predict(X_Test1)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(Y_Test1, rf_predictions))\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(Y_Test1, rf_predictions))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score (training): 0.845411\n",
            "Confusion Matrix:\n",
            "[[115  42]\n",
            " [ 22 235]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cancelled       0.84      0.73      0.78       157\n",
            "     Current       0.85      0.91      0.88       257\n",
            "\n",
            "    accuracy                           0.85       414\n",
            "   macro avg       0.84      0.82      0.83       414\n",
            "weighted avg       0.84      0.85      0.84       414\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZeTlWqy3uHE",
        "colab_type": "text"
      },
      "source": [
        "**Feature selection: Wrapper method - Random Forest**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhsGusZy30dj",
        "colab_type": "code",
        "outputId": "314d9a40-63ec-40d5-baa5-fa4f9049b4b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "# Build RF classifier to use in feature selection\n",
        "clf = RandomForestClassifier()\n",
        "\n",
        "# Build step forward feature selection\n",
        "sfs1 = sfs(clf,\n",
        "           k_features=5,\n",
        "           forward=True,\n",
        "           floating=False,\n",
        "           verbose=2,\n",
        "           scoring='accuracy',\n",
        "           cv=5)\n",
        "\n",
        "# Perform SFFS\n",
        "sfs1 = sfs1.fit(DF_TF_IDF,Y_Train)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 354 out of 354 | elapsed:   28.9s finished\n",
            "\n",
            "[2019-12-14 06:19:51] Features: 1/5 -- score: 0.6159469643320449[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 353 out of 353 | elapsed:   29.2s finished\n",
            "\n",
            "[2019-12-14 06:20:20] Features: 2/5 -- score: 0.6202889643988454[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 352 out of 352 | elapsed:   29.3s finished\n",
            "\n",
            "[2019-12-14 06:20:50] Features: 3/5 -- score: 0.6236682787577302[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 351 out of 351 | elapsed:   29.4s finished\n",
            "\n",
            "[2019-12-14 06:21:19] Features: 4/5 -- score: 0.62656916890991[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 350 out of 350 | elapsed:   29.8s finished\n",
            "\n",
            "[2019-12-14 06:21:49] Features: 5/5 -- score: 0.6289846278471081"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9JiMb0h4H8J",
        "colab_type": "code",
        "outputId": "6f42a7b4-9a07-4f76-dc0e-264c1bb45fad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Which features?\n",
        "feat_cols = list(sfs1.k_feature_idx_)\n",
        "print(feat_cols)\n"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[14, 30, 155, 295, 342]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxdFv1JjDIRO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "f5f3c7e4-b708-4d88-ca1f-208dcaa16fda"
      },
      "source": [
        "SF=DF_TF_IDF.iloc[:,feat_cols]\n",
        "print(SF)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      14   30   155  295  342\n",
            "0     0.0  0.0  0.0  0.0  0.0\n",
            "1     0.0  0.0  0.0  0.0  0.0\n",
            "2     0.0  0.0  0.0  0.0  0.0\n",
            "3     0.0  0.0  0.0  0.0  0.0\n",
            "4     0.0  0.0  0.0  0.0  0.0\n",
            "...   ...  ...  ...  ...  ...\n",
            "2065  0.0  0.0  0.0  0.0  0.0\n",
            "2066  0.0  0.0  0.0  0.0  0.0\n",
            "2067  0.0  0.0  0.0  0.0  0.0\n",
            "2068  0.0  0.0  0.0  0.0  0.0\n",
            "2069  0.0  0.0  0.0  0.0  0.0\n",
            "\n",
            "[2070 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mv5ycsInRpXC",
        "colab_type": "code",
        "outputId": "da4d17f7-abe7-46a9-c553-f4a306af3e19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "#Merge files\n",
        "\n",
        "print(CustInfoData.shape)\n",
        "\n",
        "combined=pd.concat([CustInfoData,SF], axis=1)\n",
        "print(combined.shape)\n",
        "print(combined)\n",
        "#export_csv= combined.to_csv(r'/gdrive/My Drive/CIS508/Combined-Cust+TFIDF+SelectedFeatures.csv')\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2070, 17)\n",
            "(2070, 22)\n",
            "        ID Sex Status  Children  Est_Income  ...   14   30  155  295  342\n",
            "0        1   F      S         1    38000.00  ...  0.0  0.0  0.0  0.0  0.0\n",
            "1        6   M      M         2    29616.00  ...  0.0  0.0  0.0  0.0  0.0\n",
            "2        8   M      M         0    19732.80  ...  0.0  0.0  0.0  0.0  0.0\n",
            "3       11   M      S         2       96.33  ...  0.0  0.0  0.0  0.0  0.0\n",
            "4       14   F      M         2    52004.80  ...  0.0  0.0  0.0  0.0  0.0\n",
            "...    ...  ..    ...       ...         ...  ...  ...  ...  ...  ...  ...\n",
            "2065  3821   F      S         0    78851.30  ...  0.0  0.0  0.0  0.0  0.0\n",
            "2066  3822   F      S         1    17540.70  ...  0.0  0.0  0.0  0.0  0.0\n",
            "2067  3823   F      M         0    83891.90  ...  0.0  0.0  0.0  0.0  0.0\n",
            "2068  3824   F      M         2    28220.80  ...  0.0  0.0  0.0  0.0  0.0\n",
            "2069  3825   F      S         0    28589.10  ...  0.0  0.0  0.0  0.0  0.0\n",
            "\n",
            "[2070 rows x 22 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpIZt6ZrR5uH",
        "colab_type": "code",
        "outputId": "4f6c3140-d69b-444e-f8e6-78ddba7dbaa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#Do one Hot encoding for categorical features\n",
        "categorical_features = [\"Sex\",\"Status\",\"Car_Owner\",\"Paymethod\",\"LocalBilltype\",\"LongDistanceBilltype\"]\n",
        "#X_cat = combined.select_dtypes(exclude=['int','float64'])\n",
        "print(categorical_features)\n",
        "combined_one_hot = pd.get_dummies(combined,columns=categorical_features)\n",
        "print(combined_one_hot.shape)\n",
        "export_csv= combined_one_hot.to_csv(r'/gdrive/My Drive/CIS508/Assignment-5/combined_one_hot.csv')"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Sex', 'Status', 'Car_Owner', 'Paymethod', 'LocalBilltype', 'LongDistanceBilltype']\n",
            "(2070, 30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVajDPfdA2f_",
        "colab_type": "text"
      },
      "source": [
        "**Feature selection: Wrapper method:  Splitting Train and Test data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIsSdhbhR-zF",
        "colab_type": "code",
        "outputId": "7bd63712-eb0d-45fa-b3b1-3eafd8c42c18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "source": [
        "X_Train2, X_Test2, Y_Train2, Y_Test2 = train_test_split(combined_one_hot.drop(columns=[\"TARGET\"]), \n",
        "                                                        combined_one_hot[\"TARGET\"], \n",
        "                                                        test_size=0.20, random_state=42)\n",
        "print('Training dataset shape:', X_Train2.shape, Y_Train2)\n",
        "print('Testing dataset shape:', X_Test2.shape, Y_Test2)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training dataset shape: (1656, 29) 849       Current\n",
            "1043    Cancelled\n",
            "175       Current\n",
            "1228      Current\n",
            "538     Cancelled\n",
            "          ...    \n",
            "1638      Current\n",
            "1095      Current\n",
            "1130      Current\n",
            "1294      Current\n",
            "860       Current\n",
            "Name: TARGET, Length: 1656, dtype: object\n",
            "Testing dataset shape: (414, 29) 1181      Current\n",
            "69        Current\n",
            "351     Cancelled\n",
            "1163    Cancelled\n",
            "429       Current\n",
            "          ...    \n",
            "1532      Current\n",
            "1671      Current\n",
            "416       Current\n",
            "2023      Current\n",
            "1428      Current\n",
            "Name: TARGET, Length: 414, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tf8awJn4Tm1",
        "colab_type": "code",
        "outputId": "34402798-2c4a-4363-9a12-cecbd989a8b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "#Construct a Random Forest Classifier on text data\n",
        "clf=RandomForestClassifier()\n",
        "RF_text = clf.fit(X_Train2,Y_Train2)\n",
        "print(\"Accuracy score (training): {0:.6f}\".format(clf.score(X_Test2, Y_Test2)))\n",
        "rf_predictions = clf.predict(X_Test2)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(Y_Test2, rf_predictions))\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(Y_Test2, rf_predictions))\n",
        "#run cross-validation on best hyperparameters\n",
        "clf_cv_score = cross_val_score(clf, X_Train2, Y_Train2, cv=8, scoring=\"roc_auc\")\n",
        "print(\"=== All AUC Scores ===\")\n",
        "print(clf_cv_score)\n",
        "print('\\n')\n",
        "print(\"=== Mean AUC Score ===\")\n",
        "print(\"Mean AUC Score - Random Forest: \",clf_cv_score.mean())"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score (training): 0.859903\n",
            "Confusion Matrix:\n",
            "[[128  29]\n",
            " [ 29 228]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cancelled       0.82      0.82      0.82       157\n",
            "     Current       0.89      0.89      0.89       257\n",
            "\n",
            "    accuracy                           0.86       414\n",
            "   macro avg       0.85      0.85      0.85       414\n",
            "weighted avg       0.86      0.86      0.86       414\n",
            "\n",
            "=== All AUC Scores ===\n",
            "[0.9140663  0.9170096  0.91999804 0.93337253 0.9356261  0.94503233\n",
            " 0.91024887 0.89489087]\n",
            "\n",
            "\n",
            "=== Mean AUC Score ===\n",
            "Mean AUC Score - Random Forest:  0.9212805810298018\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERJ6BGl3B5GS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "ffa3d833-710f-4406-a59f-704049194ff3"
      },
      "source": [
        "#Construct a Decision Tree Classifier on text data\n",
        "clf=DecisionTreeClassifier()\n",
        "DT_text = clf.fit(X_Train2,Y_Train2)\n",
        "print(\"Accuracy score (training): {0:.6f}\".format(clf.score(X_Test2, Y_Test2)))\n",
        "dt_predictions = clf.predict(X_Test2)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(Y_Test2, dt_predictions))\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(Y_Test2, dt_predictions))\n",
        "#run cross-validation on best hyperparameters\n",
        "clf_cv_score = cross_val_score(clf, X_Train2, Y_Train2, cv=8, scoring=\"roc_auc\")\n",
        "print(\"=== All AUC Scores ===\")\n",
        "print(clf_cv_score)\n",
        "print('\\n')\n",
        "print(\"=== Mean AUC Score ===\")\n",
        "print(\"Mean AUC Score - Decision Tree: \",clf_cv_score.mean())"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score (training): 0.801932\n",
            "Confusion Matrix:\n",
            "[[119  38]\n",
            " [ 44 213]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cancelled       0.73      0.76      0.74       157\n",
            "     Current       0.85      0.83      0.84       257\n",
            "\n",
            "    accuracy                           0.80       414\n",
            "   macro avg       0.79      0.79      0.79       414\n",
            "weighted avg       0.80      0.80      0.80       414\n",
            "\n",
            "=== All AUC Scores ===\n",
            "[0.81578692 0.84435626 0.85493827 0.82495591 0.84082892 0.77910053\n",
            " 0.81305115 0.85525794]\n",
            "\n",
            "\n",
            "=== Mean AUC Score ===\n",
            "Mean AUC Score - Decision Tree:  0.8285344865745948\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}